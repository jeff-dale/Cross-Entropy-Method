{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Entropy Method\n",
    "A Monte Carlo approach to combinatorial and continuous multi-extremal optimization and importance sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib; matplotlib.use(\"tkagg\")\n",
    "from matplotlib import animation\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Entropy Class\n",
    "Contains methods for performing optimization using the Cross Entropy Method and for plotting the results of the procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CrossEntropy:\n",
    "    \"\"\"\n",
    "    Optimize a function using the Cross Entropy method. Adapted from:\n",
    "    http://www.cleveralgorithms.com/nature-inspired/probabilistic/cross_entropy.html\n",
    "    \"\"\"\n",
    "\n",
    "    CURVE_DENSITY = 40\n",
    "\n",
    "\n",
    "    def __init__(self, dimension: int, bounds: np.ndarray, max_iterations: int, num_samples: int, num_update: int, learning_rate: float, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize parameters for Cross Entropy method of optimization.\n",
    "\n",
    "        :param dimension: number of dimensions to optimize.\n",
    "        :param bounds: list of tuples of upper/lower bounds. List is the same size as dimension.\n",
    "        :param max_iterations: maximum number of iterations in performing optimization.\n",
    "        :param num_samples: number of samples in Monte Carlo simulation.\n",
    "        :param num_update: number of samples to consider when updating distribution.\n",
    "        :param learning_rate: speed of convergence.\n",
    "        \"\"\"\n",
    "\n",
    "        self.dimension = dimension\n",
    "        self.bounds = bounds.astype(np.float64)\n",
    "        self.max_iterations = max_iterations\n",
    "        self.num_samples = num_samples\n",
    "        self.num_update = num_update\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # initialize results to empty, will be set after optimization\n",
    "        self.bests = []\n",
    "        self.averages = []\n",
    "\n",
    "        # kwargs for compatibility of **params in main function\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "\n",
    "    def optimize(self, objective_function: callable, do_print: bool = False) -> (np.ndarray, list):\n",
    "        \"\"\"\n",
    "        Optimizes given objective function.\n",
    "\n",
    "        :param objective_function: function to optimize\n",
    "        :param do_print: toggle output on and off\n",
    "        :return: best solution, list of individuals by iteration\n",
    "        \"\"\"\n",
    "\n",
    "        # Randomly initialize means within given bounds\n",
    "        means = self.bounds[:, 0] + np.multiply(\n",
    "            np.random.rand(self.dimension),\n",
    "            self.bounds[:, 1] - self.bounds[:, 0]\n",
    "        )\n",
    "\n",
    "        # Initialize standard deviations as range of bounds\n",
    "        standard_deviations = self.bounds[:, 1] - self.bounds[:, 0]\n",
    "        standard_deviations = standard_deviations.astype(np.float64)\n",
    "\n",
    "        # Initialize current best to None\n",
    "        best_individual = None\n",
    "        best_cost = None\n",
    "\n",
    "        # Intialize stuff for plotting\n",
    "        best_by_iteration = []\n",
    "        average_by_iteration = []\n",
    "\n",
    "        samples_by_iteration = []\n",
    "\n",
    "        # Iterate until convergence or until max iterations is reached\n",
    "        for iteration in range(self.max_iterations):\n",
    "\n",
    "            # Generate samples from a Gaussian distribution between bounds\n",
    "            samples = np.asarray(\n",
    "                [[max(min(np.random.normal(means[i], standard_deviations[i]), self.bounds[i, 1]), self.bounds[i, 0]) for i in range(self.dimension)] for _ in range(self.num_samples)]\n",
    "            )\n",
    "            sample_costs = objective_function(samples)\n",
    "\n",
    "            # Sort samples and sample costs by index\n",
    "            sorted_indices = sample_costs.argsort()\n",
    "            samples = samples[sorted_indices]\n",
    "            sample_costs = sample_costs[sorted_indices]\n",
    "\n",
    "            # Update best individual if we have discovered a new best\n",
    "            if best_individual is None or best_cost > sample_costs[0]:\n",
    "                best_individual, best_cost = samples[0], sample_costs[0]\n",
    "\n",
    "            # Select individuals to influence distribution\n",
    "            selected_individuals = samples[:self.num_update]\n",
    "\n",
    "            # Update distribution parameters\n",
    "            for i in range(self.dimension):\n",
    "                means[i] = self.learning_rate * means[i] + ((1.0-self.learning_rate) * np.mean(selected_individuals[:, i]))\n",
    "                standard_deviations[i] = self.learning_rate * standard_deviations[i] + ((1.0-self.learning_rate) * np.std(selected_individuals[:, i]))\n",
    "\n",
    "            # Display status of algorithm\n",
    "            if do_print:\n",
    "                print(\"Iteration {iteration}\\t\\tBest cost: {fitness}\\t\\tBest individual: {individual}\".format(\n",
    "                    iteration=str.zfill(str(iteration), 3),\n",
    "                    fitness=repr(best_cost),\n",
    "                    individual=repr(best_individual)\n",
    "                ))\n",
    "\n",
    "            samples_by_iteration.append(samples)\n",
    "            best_by_iteration.append(best_cost)\n",
    "            average_by_iteration.append(np.mean(sample_costs))\n",
    "\n",
    "        self.bests.append(best_by_iteration)\n",
    "        self.averages.append(average_by_iteration)\n",
    "\n",
    "        return best_individual, samples_by_iteration\n",
    "\n",
    "\n",
    "    def plot_reps(self, title, random_search_avg: list = None, random_search_best: list = None) -> None:\n",
    "        \"\"\"\n",
    "        Plot all reps with global best solutions.\n",
    "\n",
    "        :param title: suptitle of plot\n",
    "        :param random_search_avg: average fitness value for random search across generations\n",
    "        :param random_search_best: best fitness value for random search by generation\n",
    "        :return: void\n",
    "        \"\"\"\n",
    "\n",
    "        is_random_search = not (random_search_avg is None or random_search_best is None)\n",
    "\n",
    "        plt.subplots(1, 2, figsize=(16, 8))\n",
    "        plt.suptitle(title)\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.title(\"Average Cost by Iteration\")\n",
    "        plt.xlabel(\"Iteration\")\n",
    "        plt.ylabel(\"Average Cost\")\n",
    "\n",
    "        num_reps = len(self.averages)\n",
    "\n",
    "        # plot each rep\n",
    "        for rep in range(num_reps):\n",
    "            plt.plot(range(len(self.averages[rep])), self.averages[rep], label=\"Rep %d Average\" % (rep + 1))\n",
    "\n",
    "        if is_random_search:\n",
    "            plt.plot(range(len(random_search_avg)), random_search_avg, label=\"Random Search Average\")\n",
    "            # plt.plot(range(len(random_search_best)), random_search_best, label=\"Random Search Best\")\n",
    "\n",
    "        plt.legend(loc=\"upper left\")\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.title(\"Lowest Cost by Iteration\")\n",
    "        plt.xlabel(\"Iteration\")\n",
    "        plt.ylabel(\"Best Cost\")\n",
    "\n",
    "        # plot each rep\n",
    "        for rep in range(num_reps):\n",
    "            plt.plot(range(len(self.bests[rep])), self.bests[rep], label=\"Rep %d Best\" % (rep + 1))\n",
    "\n",
    "        if is_random_search:\n",
    "            plt.plot(range(len(random_search_best)), random_search_best, label=\"Random Search Best\")\n",
    "\n",
    "        plt.legend(loc=\"upper left\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Search class\n",
    "Simply class for performing a random search of the given space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RandomSearch:\n",
    "\n",
    "\n",
    "    def __init__(self, dimension: int, bounds: np.ndarray, max_iterations: int, num_samples: int, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize parameters for Random Search benchmark.\n",
    "\n",
    "        :param dimension: number of dimensions to optimize.\n",
    "        :param bounds: list of tuples of upper/lower bounds. List is the same size as dimension.\n",
    "        :param max_iterations: maximum number of iterations in performing optimization.\n",
    "        :param num_samples: number of samples in Monte Carlo simulation.\n",
    "        :param num_update: number of samples to consider when updating distribution.\n",
    "        :param learning_rate: speed of convergence.\n",
    "        \"\"\"\n",
    "\n",
    "        self.dimension = dimension\n",
    "        self.bounds = bounds.astype(np.float64)\n",
    "        self.max_iterations = max_iterations\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "        # unused, just for compatibility with initializing CE and RS with same **params\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "\n",
    "    def optimize(self, objective_function: callable, do_print: bool = False) -> (np.ndarray, list):\n",
    "        \"\"\"\n",
    "        Optimizes given objective function.\n",
    "\n",
    "        :param objective_function: function to optimize\n",
    "        :return: best solution, list of individuals by iteration\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize current best to None\n",
    "        best_individual = None\n",
    "        best_cost = None\n",
    "\n",
    "        # Intialize stuff for plotting\n",
    "        best_by_iteration = []\n",
    "        average_by_iteration = []\n",
    "\n",
    "        samples_by_iteration = []\n",
    "\n",
    "\n",
    "        # Iterate until convergence or until max iterations is reached\n",
    "        for iteration in range(self.max_iterations):\n",
    "\n",
    "            # Generate samples from a Gaussian distribution between bounds\n",
    "            samples = np.asarray(\n",
    "                [[max(min(self.bounds[i, 0] + np.random.rand()*(self.bounds[i, 1] - self.bounds[i, 0]), self.bounds[i, 1]), self.bounds[i, 0]) for\n",
    "                  i in range(self.dimension)] for _ in range(self.num_samples)]\n",
    "            )\n",
    "            sample_costs = objective_function(samples)\n",
    "\n",
    "            # Sort samples and sample costs by index\n",
    "            sorted_indices = sample_costs.argsort()\n",
    "            samples = samples[sorted_indices]\n",
    "            sample_costs = sample_costs[sorted_indices]\n",
    "\n",
    "            # Update best individual if we have discovered a new best\n",
    "            if best_individual is None or best_cost > sample_costs[0]:\n",
    "                best_individual, best_cost = samples[0], sample_costs[0]\n",
    "\n",
    "            # Display status of algorithm\n",
    "            if do_print:\n",
    "                print(\"Iteration {iteration}\\t\\tBest cost: {fitness}\\t\\tBest individual: {individual}\".format(\n",
    "                    iteration=str.zfill(str(iteration), 3),\n",
    "                    fitness=repr(best_cost),\n",
    "                    individual=repr(best_individual)\n",
    "                ))\n",
    "\n",
    "            samples_by_iteration.append(samples)\n",
    "            best_by_iteration.append(best_cost)\n",
    "            average_by_iteration.append(np.mean(sample_costs))\n",
    "\n",
    "\n",
    "        return best_individual, samples_by_iteration, best_by_iteration, average_by_iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark Functions\n",
    "Below are several functions that are popular in benchmarking single-objective optimization algorithms. These functions came from the page https://en.wikipedia.org/wiki/Test_functions_for_optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rastrigin Function\n",
    "![Rastrigin Function](img/Rastrigin_function.png)\n",
    "Defined by\n",
    "$$f(x) = An + \\sum_{i=1}^{n} \\left[ x_i^2 - A\\cos{(2\\pi x_i)}\\right]$$\n",
    "In this instance, we use $A = 10$ and $n=2$ for 2-dimensional vector inputs. The global minimum of the Rastrigin function is $f(\\textbf{0})=0$. The recommended search domain is $-5.12\\le x, y \\le 5.12$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Rastrigin(x: np.ndarray, A: float = 10, n: int = 2) -> float:\n",
    "    \"\"\"\n",
    "    Benchmark function with many local optima.\n",
    "    Global minimum is at x = 0.\n",
    "    Bounds: (-5.12, 5.12)\n",
    "\n",
    "    :param x: input vector\n",
    "    :param A: arbitrary constant, typically A=10\n",
    "    :param n: number of dimensions\n",
    "    :return: value of Rastrigin function\n",
    "    \"\"\"\n",
    "    total = np.ones((x.shape[0]))*A*n\n",
    "    for i in range(n):\n",
    "        total += (np.power(x[:, i], 2) - A*np.cos(2*np.pi*x[:, i]))\n",
    "    return total.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ackley's Function\n",
    "![Ackley's Function](img/ackley.jpg)\n",
    "Defined by\n",
    "$$f(x, y) = -20\\exp{\\left[-0.2\\sqrt{0.5(x^2+y^2)}\\right]}-\\exp{\\left[0.5(\\cos{(2\\pi x)} + \\cos{(2\\pi y)}\\right]} + e + 20$$\n",
    "The global minimum of Ackley's function is $f(0, 0)=0$. The recommended search domain is $-5\\le x, y \\le 5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Ackley(x: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Ackley function for benchmarking.\n",
    "    Global minimum at (0, 0).\n",
    "    Bounds: (-5, 5)\n",
    "\n",
    "    :param x: input vector, must be 2D\n",
    "    :return: value of Ackley function\n",
    "    \"\"\"\n",
    "    return -20*np.exp(-0.2*np.sqrt(0.5*(np.power(x[:, 0], 2) + np.power(x[:, 1], 2)))) - \\\n",
    "        np.exp(0.5*(np.cos(2*np.pi*x[:, 0]) + np.cos(2*np.pi*x[:, 1]))) + np.e + 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Easom Function\n",
    "![Easom Function](img/easom.jpg)\n",
    "Defined by\n",
    "$$f(x, y) = -\\cos{(x)}\\cos{(y)}\\exp{\\left(-\\left((x-\\pi)^2 + (y-\\pi)^2\\right)\\right)}$$\n",
    "The global minimum of the Easom's function is $f(\\pi, \\pi)=-1$. The recommended search domain is $-100\\le x, y \\le 100$. This function really tests the ability of the optimization algorithm to thoroughly sample the search space, as no solution is distinct aside from those inside the hole around $(\\pi, \\pi)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Easom(x: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Easom function for benchmarking.\n",
    "    Global minimum at f(pi, pi) = -1\n",
    "    Bounds: (-100, 100)\n",
    "\n",
    "    :param x: input vector, must be 2D\n",
    "    :return: value of Easom function\n",
    "    \"\"\"\n",
    "    return -np.cos(x[:, 0])*np.cos(x[:, 1])*np.exp(-(np.power(x[:, 0] - np.pi, 2) + np.power(x[:, 1] - np.pi, 2)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-in-Tray Function\n",
    "![Cross-in-Tray Function](img/crosstray.jpg)\n",
    "Defined by\n",
    "$$f(x, y) = -0.0001\\left[\\left\\lvert \\sin{x} \\sin{y} \\exp\\left( \\left\\lvert 100 - \\frac{\\sqrt{x^2+y^2}}{\\pi} \\right\\rvert \\right) \\right\\rvert+ 1\\right]^{0.1}$$\n",
    "The global minimum of the cross-in-tray function is $f(\\pm 1.34941, \\pm 1.34941) = -2.06261$. The recommended search domain is $-10\\le x, y \\le 10$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Cross_in_Tray(x: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Cross-in-tray function for benchmarking.\n",
    "    Global minimums at f(+- 1.34941, +-1.34941) = -2.06261\n",
    "    Bounds: (-10, 10)\n",
    "\n",
    "    :param x: input vector, must be 2D\n",
    "    :return: value of Cross_in_Tray function\n",
    "    \"\"\"\n",
    "    return -0.0001*np.power(np.abs(np.sin(x[:, 0])*np.sin(x[:, 1])*np.exp(np.abs(100-np.sqrt(np.power(x[:, 0], 2) + np.power(x[:, 1], 2))/np.pi))) + 1, 0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Levi Function\n",
    "![Levi Function](img/levi.jpg)\n",
    "Defined by\n",
    "$$f(x, y) = \\sin^2(3\\pi x) + (x-1)^2(1+\\sin^2(3\\pi y) + (y-1)^2(1+\\sin^2(2\\pi y)$$\n",
    "The global minimum of the Levi function is $f(1, 1) = 0$. The recommended search domain is $-10\\le x, y \\le 10$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Levi(x: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Levi function for benchmarking.\n",
    "    Global minimum at f(0, 0) = 0\n",
    "    Bounds: (-5, 5)\n",
    "    \n",
    "    :param x: input vector, must be 2D\n",
    "    :return: value of Levi function\n",
    "    \"\"\"\n",
    "    return np.power(np.sin(3*np.pi*x[:, 0]), 2) + np.multiply(np.power(x[:, 0] - 1, 2), 1+np.power(np.sin(3*np.pi*x[:, 1]), 2))+np.multiply(np.power(x[:, 1]-1, 2), 1+np.power(np.sin(2*np.pi*x[:, 1]), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def animate(objective_function: callable, samples_by_iteration, bounds: np.ndarray, curve_density: int):\n",
    "\n",
    "    # Precalculate surface points for plotting\n",
    "    xs = np.linspace(bounds[0][0], bounds[0][1], curve_density).reshape(\n",
    "        (1, curve_density))\n",
    "    ys = np.linspace(bounds[1][0], bounds[1][1], curve_density).reshape(\n",
    "        (1, curve_density))\n",
    "    X, Y = np.meshgrid(xs, ys)\n",
    "    Z = objective_function(np.vstack((np.ravel(X), np.ravel(Y))).T).reshape(X.shape)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    samples = fig.add_subplot(111, projection=\"3d\")\n",
    "    samples.plot_surface(X, Y, Z, color=(1, 1, 0, 0.5))\n",
    "    individuals = samples.scatter(samples_by_iteration[0][:, 0], samples_by_iteration[0][:, 1], objective_function(samples_by_iteration[0]), c=(0, 0, 0), s=100)\n",
    "\n",
    "\n",
    "    def init():\n",
    "        return samples,\n",
    "\n",
    "    def iterate(i):\n",
    "        nonlocal individuals\n",
    "        individuals.remove()\n",
    "        individuals = samples.scatter(samples_by_iteration[i][:, 0], samples_by_iteration[i][:, 1], objective_function(samples_by_iteration[i]), c=(0, 0, 0), s=100)\n",
    "        return samples,\n",
    "\n",
    "    anim = animation.FuncAnimation(fig, iterate, init_func=init, frames=len(samples_by_iteration), interval=40, blit=False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective_functions = {\n",
    "    \"Sphere\": {\"f\": lambda x: np.sum((np.power(x, 2)), axis=1), \"bounds\": np.asarray([[-5, 5], [-5, 5]])},\n",
    "    \"Rastrigin\": {\"f\": Rastrigin, \"bounds\": np.asarray([[-5, 5], [-5, 5]])},\n",
    "    \"Ackley\": {\"f\": Ackley, \"bounds\": np.asarray([[-5.12, 5.12], [-5.12, 5.12]])},\n",
    "    \"Easom\": {\"f\": Easom, \"bounds\": np.asarray([[-100, 100], [-100, 100]])},\n",
    "    \"Cross in Tray\": {\"f\": Cross_in_Tray, \"bounds\": np.asarray([[-10, 10], [-10, 10]])},\n",
    "    \"Levi\": {\"f\": Levi, \"bounds\": np.asarray([[-10, 10], [-10, 10]])}\n",
    "}\n",
    "\n",
    "objective_function = \"Levi\"\n",
    "\n",
    "params = {\n",
    "    \"dimension\": 2,\n",
    "    \"bounds\": objective_functions[objective_function][\"bounds\"],\n",
    "    \"max_iterations\": 100,\n",
    "    \"num_samples\": 50,\n",
    "    \"num_update\": 5,\n",
    "    \"learning_rate\": 0.7,\n",
    "    \"num_reps\": 1\n",
    "}\n",
    "\n",
    "CE = CrossEntropy(**params)\n",
    "RS = RandomSearch(**params)\n",
    "\n",
    "RS_best, RS_samples_by_iteration, RS_best_by_iteration, RS_average_by_iteration = RS.optimize(objective_functions[objective_function][\"f\"])\n",
    "CE_bests, CE_averages = [], []\n",
    "\n",
    "CE_best, CE_samples_by_iteration = None, None\n",
    "for rep in range(params[\"num_reps\"]):\n",
    "    CE_best, CE_samples_by_iteration = CE.optimize(objective_functions[objective_function][\"f\"], do_print=False)\n",
    "\n",
    "animate(objective_functions[objective_function][\"f\"], CE_samples_by_iteration, params[\"bounds\"], CrossEntropy.CURVE_DENSITY)\n",
    "#CE.plot_reps(\"Cross Entropy on %s Function (Î±=%.2f)\" % (objective_function, params[\"learning_rate\"]), RS_average_by_iteration, RS_best_by_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
